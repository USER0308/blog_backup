# 面向对象的爬虫小demo

> 国庆期间闲来无事，又回不得家，网上冲浪之际发现一个高清屏幕壁纸网站，于是想着更换壁纸，就把壁纸 download 下来，但又不想手动操作，遂写爬虫。

## 分析网页

* 首页是 http://www.XXXXXX.com，

* 网站有各种壁纸 tag，tag 列表的 URL 是 http://www.XXXXXX.com/tag/,

* 点开一个 tag， 比如说“自然风光”，URL 为 http://www.XXXXXX.com/tag/nature.html, 下面有许多套图，例如“江南水乡”，“大漠风沙”，

* 一个套图里面有几张到几十张数量不等的壁纸，这些壁纸分页显示，每页4张，

* 首页 URL 以 随机数字 + ‘.html’ 结尾，自第二页始，页面 URL变成 该随机数字 + ‘下划线’ + '页码' + ’.html‘ 结尾，

* 每页的标题都会以’(当期页面/所有页码)’的形式标明当前页码以及所有页码，每页的四张壁纸的 html 代码格式为

  ```
<img alt="WWWWWWWW" src="http://t1.XXXXXX.com/uploads/allimg/YYYYYY/ZZZZZZ.jpg" style="display: none;">

  ```

至此,网页分析完毕。

## 设计

设计分为 *自底向上* 和 *自顶向下* 两种。

自顶向下是说先从主页入手，使得爬虫模拟人的点击行为，从 ‘main’ - ‘tag’ - ’suit’ - ‘picture’，然后 download 壁纸。

自底向上是说先 download 一张壁纸，然后 download 一页壁纸，然后是一套，一个标签，所有标签。

在本次中，我用的是自顶向下，先获得主页所有的 tag 的 URL， 将其保存到一个 tag_list 中， 依次遍历，取出 tag 的 URL, 再获得所有 suit 的 URL， 将其保存到一个 suit_list, 依次遍历，取出 suit 的 URL， 获得所有 picture 的 URL， 最后 download 即可。

## 类

```
main.py  // 传入主页地址，初始化爬虫，启动爬虫
|--model
|----|--__init__.py
|----|--spider.py  // 爬虫类，获得所有 tag 的 URL
|----|--tag.py  // 标签类，获得指定标签下所有的 suit 的 URL
|----|--suit.py  // 套图类，获得指定套图下所有的 picture 的 URL， 爬取并保存 picture
```

## 代码

### main.py

```
#! /usr/bin/python
# *-* encoding:utf-8 *-*
from model import Spider

if __name__ == "__main__":
    spider = Spider("http://www.XXXXXX.com")
    spider.init()
    spider.spy()
```

### model.\_\_init\_\_.py

```
from spider import Spider
from tag import Tags
from suit import Suit
```

### model.spider.py

```
# *-* encoding:utf-8*-*
import urllib
import os
from bs4 import BeautifulSoup

from tag import Tags


class Spider:
    host = ''
    tag_page = ''
    tags_list = []

    def __init__(self,host):
        self.host = host
        self.tag_page = host + '/tag/'
        #print self.tag_page

    def init(self):
        response = urllib.urlopen(self.tag_page)
        content = response.read()
        soup = BeautifulSoup(content,"html.parser")
        tag_list = soup.find_all('dd')
        for i in range(0,5):
            categories = tag_list[i].select('a[href^="/tag/"]')
            for category in categories:
                # print category['href']
                full_url = self.host + category['href']
                self.tags_list.append(full_url)

        print "total tags are:"
        for url in self.tags_list:
            print url

    def spy(self):
        if os.path.exists('download'):
            pass
        else:
            os.mkdir('download')
        os.chdir('download')
        tag_num = 1
        for url in self.tags_list:
            print "正在爬取第 %s/%s 个标签下的套图" %(str(tag_num),str(len(self.tags_list)))
            tag_num += 1
            tag = Tags(url)
            tag.init()
            tag.start()
```

### model.tag.py

```
# *-* encoding:utf-8*-*
import urllib
import re
import os
from bs4 import BeautifulSoup

from suit import Suit


class Tags:
    tag = ''
    index_url = ''
    page_of_suit = 1
    url_suit_list = []

    def __init__(self,index_url):
        self.index_url = index_url

    def init(self):
        self.url_suit_list = []
        response = urllib.urlopen(self.index_url)
        content = response.read()
        soup = BeautifulSoup(content, "html.parser")
        head = soup.find('h1')
        print 'Tag is ', head.string
        self.tag = head.string
        #tag_filter = 'a[href^="/tag/%s/"]' %self.tag
        selector = soup.select('a[href^="/tag/"]')
        # print selector
        tail_url = selector[len(selector)-1]
        reg = re.compile(r'/tag/.*?/(\d+?).html')
        reg_match = reg.match(tail_url['href'])
        if reg_match is None:
            self.page_of_suit = 1
            return
        grp = reg_match.groups()
        if grp.__len__() == 1:
            self.page_of_suit = int(grp[0])
            # print self.page_of_suit
        else:
            print "match length not equals 1 !"
            return

    def start(self):
        if os.path.exists(self.tag):
            print "tags exists !!!"
        else:
            os.mkdir(self.tag)
        os.chdir(self.tag)
        print '该标签下共有' + str(self.page_of_suit) + '页'
        cut_url = self.index_url[:-5]
        page_url_list = []
        page_url_list.append(self.index_url)
        print "该标签下 index_url is ",self.index_url
        if self.page_of_suit >= 2:
            for i in range(2, int(self.page_of_suit)+1):
                tmp_url = cut_url + '/' + str(i) + '.html'
                print "该标签下第 %s 页 URL is " %str(i)
                print tmp_url
                page_url_list.append(tmp_url)

        page = 1
        for url_item in page_url_list:
            print '正在爬取第 %s/%s 页的套图 URL' %(str(page),str(self.page_of_suit))
            self.crawl(url_item)
            print '爬取第 %s/%s 页的套图 URL 完成' %(str(page),str(len(page_url_list)))
            if page < self.page_of_suit:
                page += 1
            else:
                print '%s 页的套图 URL 全部爬取完毕,开始爬取套图' %(str(self.page_of_suit))
        suit_num = 1
        for url_item in self.url_suit_list:
            print '正在爬取第 %s/%s 套图' %(str(suit_num),str(len(self.url_suit_list)))
            suit = Suit(url_item)
            suit.init()
            suit.start()
            print '爬取第 %s/%s 套图完成' %(str(suit_num),str(len(self.url_suit_list)))
            suit_num +=1
            if page < len(self.url_suit_list):
                page += 1
            else:
                print '%s 套图全部爬取完毕' %(str(page))
        os.chdir("..")

    def crawl(self,crawl_url):
        response = urllib.urlopen(crawl_url)
        content = response.read()
        soup = BeautifulSoup(content, "html.parser")
        img_selector = soup.select('a[class="like-img-text"]')
        #print 'selector is ',soup

        print "该标签在该页的套图 URL 有 ："
        for img_item in img_selector:
            print img_item['href']
            self.url_suit_list.append(img_item['href'])
            #print type(img_item['href'])
```

### model.suit.py

```
# *-* encoding:utf-8*-*
import urllib
import re
import os
from bs4 import BeautifulSoup


class Suit:
    name = '' # name
    index_url = '' # root url
    page_num = 0 # number of page
    url_list = [] # url of each picture


    def __init__(self,index_url):
        self.index_url = index_url

    def init(self):
        response = urllib.urlopen(self.index_url)
        content = response.read()
        soup = BeautifulSoup(content, "html.parser")
        head_string = soup.title.string
        print '套图标题 is ', head_string
        reg = re.compile(r'(.+?)\( \d+/(\d+) \).+?')
        reg_match = reg.match(head_string)
        grp =  reg_match.groups()
        if grp.__len__() == 2:
            self.name = grp[0]
            self.page_num = int(grp[1])
        else:
            print "title length not equals 2 !"
            return


    def start(self):
        # contract the str into a full url
        cut_url = self.index_url[:-5]
        page_url_list = []
        page_url_list.append(self.index_url)
        for i in range(2,self.page_num+1):
            tmp_url = cut_url + '_' + str(i) + '.html'
            page_url_list.append(tmp_url)
        print '共有' + str(self.page_num) + '页'

        # begin to crawl the pages
        page = 1
        for url in page_url_list:
            print '正在爬取第 %d/%d 页' %(page,self.page_num)
            self.crawl(url)
            print '爬取第 %d/%d 页完成' %(page,self.page_num)
            if page  < self.page_num:
                page += 1
            else:
                print '%d 页全部爬取完毕' %self.page_num

    def crawl(self,crawl_url):
        response = urllib.urlopen(crawl_url)
        content = response.read()
        soup = BeautifulSoup(content, "html.parser")
        img_selector =  soup.select('img[lazysrc^="http://t1.XXXXXX.com/uploads/allimg"]')

        for img_item in img_selector:
            print img_item['lazysrc']
            self.url_list.append(img_item['lazysrc'])
            self.save(img_item['lazysrc'])

    def save(self,pic_url):
        response = urllib.urlopen(pic_url)
        content = response.read()

        if os.path.exists(self.name):
            pass
        else:
            os.mkdir(self.name)

        cut_list = pic_url.split('/')
        file_name = self.name + '/' + cut_list[len(cut_list)-1]
        file = open(file_name,'wb+')
        file.write(content)
        file.close()


    def print_suit(self):
        print "name: ",self.name
        print "index_url: ",self.index_url
        print "page number: ",self.page_num
```
